# Reproducible Person Counting Evaluation on COCO (Val2017)

## Overview
This project evaluates person-counting performance under three prompting strategies:

- **Baseline**
- **Chain-of-Thought (CoT)**
- **Grounded Reasoning**

All experiments are conducted on a **fixed subset of images** from the COCO 2017 validation set to ensure reproducibility. Ground-truth counts are extracted directly from COCO annotations, while predictions are manually recorded.

---

## Dataset
- **Dataset:** MS COCO 2017 Validation Set
- **Annotations:** `instances_val2017.json`
- **Category Evaluated:** `person`

Although images were originally sampled randomly using a fixed seed, the final evaluation uses **frozen image IDs** to ensure consistency with manually collected predictions and reported results.

---

## Fixed Image Set
The following COCO image IDs are used for all experiments:
260925, 47740, 82986, 42528, 15597,
184978, 363875, 128699, 497568, 252776,
357737, 79014

---

## Experimental Pipeline

### 1. Environment Setup
The script automatically:
- Installs `pycocotools`
- Downloads COCO 2017 validation annotations
- Downloads COCO 2017 validation images (`val2017`)

This allows the experiment to be run end-to-end without manual dataset preparation.

---

### 2. Ground Truth Extraction
For each selected image:
- All annotations labeled as **person** are retrieved
- The total number of person instances is counted
- These counts are stored as ground-truth labels

---

### 3. Prediction Collection
For each image, predictions are recorded under three prompting strategies:
- `baseline`
- `cot`
- `grounded`

Predicted counts are **manually entered** after visual inspection of each image and querying the model. This design choice ensures transparency and avoids hidden automation.

---

### 4. Evaluation Metrics
Two metrics are computed for each prompting strategy:

- **Mean Absolute Error (MAE)**  
  Average absolute difference between predicted and ground-truth counts.

- **Exact Match Rate**  
  Fraction of images where the predicted count exactly matches the ground truth.

---

## Results
Results are aggregated per prompting strategy and summarized in a table showing:
- Average absolute error
- Exact match rate

A bar chart visualization is also generated for direct comparison across methods.

---

## Reproducibility Notes
- Image IDs are fixed and explicitly listed
- Ground-truth labels are derived directly from COCO annotations
- No randomness is used during evaluation
- All reported results can be regenerated by running the script

---

## Dependencies
- Python 3.x
- numpy
- pandas
- matplotlib
- pillow
- pycocotools

---

## Notes
This project is intended for **controlled comparison of reasoning strategies** in object counting tasks, not for training or fine-tuning models.

