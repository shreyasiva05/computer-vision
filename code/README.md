# Reproducible Person Counting Evaluation on COCO (Val2017)

## Overview
This project evaluates person-counting performance under three prompting strategies:

- **Baseline**
- **Chain-of-Thought (CoT)**
- **Grounded Reasoning**

All experiments are conducted on a **fixed subset of images** from the COCO 2017 validation set to ensure reproducibility. Ground-truth counts are extracted directly from COCO annotations, while predictions are manually recorded.

---

## Dataset
- **Dataset:** MS COCO 2017 Validation Set
- **Annotations:** `instances_val2017.json`
- **Category Evaluated:** `person`

Although images were originally sampled randomly using a fixed seed, the final evaluation uses **frozen image IDs** to ensure consistency with manually collected predictions and reported results.

---

## Fixed Image Set
The following COCO image IDs are used for all experiments:

