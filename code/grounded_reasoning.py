# -*- coding: utf-8 -*-
"""Grounded_reasoning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HPwsKCnrAlzQHeLlQsw-CnC4TRtyDKJv
"""

# Basic imports
import os
import json
import pandas as pd
import numpy as np

!pip install pycocotools

# Commented out IPython magic to ensure Python compatibility.
!mkdir -p coco
# %cd coco

# Download annotations
!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip
!unzip annotations_trainval2017.zip

from pycocotools.coco import COCO

ann_file = "annotations/instances_val2017.json"
coco = COCO(ann_file)

cat_name = "person"
cat_id = coco.getCatIds(catNms=[cat_name])[0]
print("Category ID:", cat_id)

# ---------------------------------------------------------
# IMAGE SELECTION (FIXED FOR REPRODUCIBILITY)
# ---------------------------------------------------------
# NOTE:
# Images were originally selected via random sampling using
# a fixed seed. However, to ensure full reproducibility and
# consistency with the manually collected predictions and
# ground-truth annotations, we freeze the image IDs below.
# These images are reused across all experiments.

# Original (commented out):
# np.random.seed(42)
# img_ids = coco.getImgIds(catIds=[CAT_ID])
# selected_imgs = np.random.choice(img_ids, size=12, replace=False)

# Fixed image IDs used in all experiments
selected_imgs = np.array([
    260925, 47740, 82986, 42528, 15597,
    184978, 363875, 128699, 497568, 252776,
    357737, 79014
])

print("Using fixed image IDs for evaluation:")
print(selected_imgs)

gt_counts = {}

for img_id in selected_imgs:
    ann_ids = coco.getAnnIds(imgIds=img_id, catIds=[cat_id])
    gt_counts[img_id] = len(ann_ids)

gt_counts

results = []

methods = ["baseline", "cot", "grounded"]

for img_id in selected_imgs:
    for m in methods:
        results.append({
            "image_id": img_id,
            "method": m,
            "predicted_count": None  # fill later
        })

df = pd.DataFrame(results)
df

df["ground_truth"] = df["image_id"].map(gt_counts)
df["abs_error"] = abs(df["predicted_count"] - df["ground_truth"])
df["exact_match"] = (df["predicted_count"] == df["ground_truth"]).astype(int)

summary = df.groupby("method").agg(
    avg_abs_error=("abs_error", "mean"),
    exact_match_rate=("exact_match", "mean")
)

summary

print(summary)
print(summary.dtypes)

df.groupby("method")["predicted_count"].apply(list)

from PIL import Image
import matplotlib.pyplot as plt

def show_coco_image(img_id, coco, img_dir="val2017"):
    img_info = coco.loadImgs(img_id)[0]
    img_path = f"{img_dir}/{img_info['file_name']}"

    img = Image.open(img_path)
    plt.figure(figsize=(6, 6))
    plt.imshow(img)
    plt.axis("off")
    plt.title(f"Image ID: {img_id}")
    plt.show()

ann_file = "annotations/instances_val2017.json"
coco = COCO(ann_file)

print(type(selected_imgs[0]), selected_imgs[0])
print(coco.loadImgs([selected_imgs[0]]))

from PIL import Image
import matplotlib.pyplot as plt

def show_coco_image(img_id, coco, img_dir="val2017"):
    img_info = coco.loadImgs([img_id])[0]  # <-- FIX
    img_path = f"{img_dir}/{img_info['file_name']}"

    img = Image.open(img_path)
    plt.figure(figsize=(6, 6))
    plt.imshow(img)
    plt.axis("off")
    plt.title(f"Image ID: {img_id}")
    plt.show()

!wget http://images.cocodataset.org/zips/val2017.zip
!unzip val2017.zip

from PIL import Image
import matplotlib.pyplot as plt
import os

def show_image_with_categories(img_id, coco, img_dir="val2017"):
    # load image info
    img_info = coco.loadImgs([img_id])[0]
    img_path = os.path.join(img_dir, img_info["file_name"])

    # load image
    img = Image.open(img_path)

    # get annotations + category names
    ann_ids = coco.getAnnIds(imgIds=[img_id])
    anns = coco.loadAnns(ann_ids)
    cat_ids = list(set([ann["category_id"] for ann in anns]))
    cat_names = [coco.loadCats([cid])[0]["name"] for cid in cat_ids]

    # display
    plt.figure(figsize=(6,6))
    plt.imshow(img)
    plt.axis("off")
    plt.title(f"Image ID: {img_id}\nCategories: {', '.join(cat_names)}")
    plt.show()

for img_id in selected_imgs:
    show_image_with_categories(img_id, coco)

import pandas as pd
import numpy as np

# --- 1. RE-ESTABLISH VARIABLES (For continuity) ---

# These are the 12 image IDs randomly selected by np.random.seed(42)
selected_imgs = np.array([
    260925,  47740,  82986,  42528,  15597, 184978,
    363875, 128699, 497568, 252776, 357737,  79014
])

# These are the Ground Truth Counts derived from COCO annotations
gt_counts = {
    260925: 1,
    47740: 4,
    82986: 3,
    42528: 1,
    15597: 1,
    184978: 1,
    363875: 1,
    128699: 12,
    497568: 2,
    252776: 3,
    357737: 4,
    79014: 1
}

# --- 2. RE-CREATE DATAFRAME (If you need to reset all predictions) ---

results = []
methods = ["baseline", "cot", "grounded"]

for img_id in selected_imgs:
    for m in methods:
        results.append({
            "image_id": img_id,
            "method": m,
            "predicted_count": None
        })

df = pd.DataFrame(results)
df["ground_truth"] = df["image_id"].map(gt_counts)
print("Initial DataFrame (Ready for filling):")
print(df.head())


# --- 3. ENTER PREDICTED COUNTS ---

# To get these counts, you must view the image (show_coco_image(img_id, coco))
# and note the number of people your model predicted for *each* method.
print("\n--- ENTERING BASELINE PREDICTIONS ---")
baseline_person_counts = {
    260925: 1,  # Example: Change this to your model's prediction for 260925 (Baseline)
    47740: 3,
    82986: 2,
    42528: 1,
    15597: 1,
    184978: 0,
    363875: 1,
    128699: 10,
    497568: 2,
    252776: 3,
    357737: 4,
    79014: 1
}

for img_id, count in baseline_person_counts.items():
    df.loc[
        (df.image_id == img_id) & (df.method == "baseline"),
        "predicted_count"
    ] = count


print("\n--- ENTERING CHAIN-OF-THOUGHT (COT) PREDICTIONS ---")
cot_person_counts = {
    260925: 1,  # Change this to your model's prediction for 260925 (COT)
    47740: 4,
    82986: 3,
    42528: 1,
    15597: 1,
    184978: 1,
    363875: 1,
    128699: 11,
    497568: 2,
    252776: 3,
    357737: 4,
    79014: 1
}

for img_id, count in cot_person_counts.items():
    df.loc[
        (df.image_id == img_id) & (df.method == "cot"),
        "predicted_count"
    ] = count


print("\n--- ENTERING GROUNDED PREDICTIONS ---")
grounded_person_counts = {
    260925: 1,  # Change this to your model's prediction for 260925 (Grounded)
    47740: 4,
    82986: 3,
    42528: 1,
    15597: 1,
    184978: 1,
    363875: 1,
    128699: 12,
    497568: 2,
    252776: 3,
    357737: 4,
    79014: 1
}

for img_id, count in grounded_person_counts.items():
    df.loc[
        (df.image_id == img_id) & (df.method == "grounded"),
        "predicted_count"
    ] = count


# --- 4. SANITY CHECK & FINAL ANALYSIS ---

# Check for missing values (Should be 0 if all 3 blocks above are filled)
missing_counts = df["predicted_count"].isna().sum()
print(f"\nTotal missing predictions after filling: {missing_counts}")
if missing_counts > 0:
    print("WARNING: You still have missing predictions. Check your dictionaries.")

# Calculate error metrics
df["abs_error"] = abs(df["predicted_count"] - df["ground_truth"])
df["exact_match"] = (df["predicted_count"] == df["ground_truth"]).astype(int)

summary = df.groupby("method").agg(
    avg_abs_error=("abs_error", "mean"),
    exact_match_rate=("exact_match", "mean")
)

print("\n--- FINAL SUMMARY RESULTS ---")
print(summary)

# Plot the results
import matplotlib.pyplot as plt

summary.plot(kind="bar", subplots=True, layout=(1, 2), figsize=(12, 5), legend=False)
plt.suptitle("Object Counting Performance by Method (12 Images)")
plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout for suptitle
plt.show()

print("\nDataFrame Head (with errors calculated):")
print(df.head())